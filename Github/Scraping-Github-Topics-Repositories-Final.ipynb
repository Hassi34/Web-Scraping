{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814c7814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac5bd3",
   "metadata": {},
   "source": [
    "# Top Repositories for Github Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79135da",
   "metadata": {},
   "source": [
    "## Project Outline :\n",
    "- In this notebook, i'll be scraping \"https://github.com/topics\"\n",
    "- I'll the list of topics. For each topic, I'll get topic title, topic page URL and topic description\n",
    "- For each topic ill get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, I'll grab the repo name , username, stars and repo URL\n",
    "- For each topic, I'll create a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeed42a",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc1bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd \n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d35522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests --upgrade --quiet\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff0e05",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Github\n",
    "\n",
    "- Use requests to downlaod a page\n",
    "- Use BS4 to parse and extract the information\n",
    "- Convert to Pandas dataframe\n",
    "\n",
    "Let's write a fuction to download the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4b5c53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>,\n",
       " <a aria-label=\"Homepage\" class=\"mr-4\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\" href=\"https://github.com/\">\n",
       " <svg aria-hidden=\"true\" class=\"octicon octicon-mark-github color-text-white\" height=\"32\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"32\"><path d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\" fill-rule=\"evenodd\"></path></svg>\n",
       " </a>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topics_page():\n",
    "    '''\n",
    "    This will get the url and provide the document of topics\n",
    "    \n",
    "    '''\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load the page {topics_url}')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc \n",
    "doc = get_topics_page()\n",
    "doc.find_all('a')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb841323",
   "metadata": {},
   "source": [
    "Let's create some helper function to parse the information form the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f91acd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_title(doc):\n",
    "    '''\n",
    "    This function will be used to get the list of titles\n",
    "    \n",
    "    '''\n",
    "    selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tags = doc.find_all(\"p\" , {'class': selection_class})\n",
    "    topic_titles = [ tag.text for tag in topic_title_tags]\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1716663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = get_topics_titles(doc)\n",
    "print(len(titles))\n",
    "titles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68892aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_description(doc):\n",
    "    '''\n",
    "    This function will be used to get the topic description\n",
    "    \n",
    "    '''\n",
    "    topic_desc_tags = doc.find_all('p', {'class' :'f5 color-text-secondary mb-0 mt-1'})\n",
    "    topic_descs = [topic_desc.text.strip() for topic_desc in topic_desc_tags]\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e45148c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3D modeling is the process of virtually developing the surface and structure of a 3D object.',\n",
       " 'Ajax is a technique for creating interactive web applications.',\n",
       " 'Algorithms are self-contained sequences that carry out a variety of tasks.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_desc = get_topics_description(doc)\n",
    "print(len(topic_desc))\n",
    "topic_desc[:3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "69f4d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(doc):\n",
    "    '''\n",
    "    This function will be used to get the topic url\n",
    "    \n",
    "    '''\n",
    "    topic_link_tag = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
    "    topic_urls = ['https://github.com'+tage['href']for tage in topic_link_tag]\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "beff52bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_url = get_topics_url(doc)\n",
    "print(len(topic_url))\n",
    "topic_url[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbfa31",
   "metadata": {},
   "source": [
    "**Now let's put it all together into a single function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66764d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    '''\n",
    "    This function will get the topics from the topics page and for each topic, it is going to get the topic name, description and url.\n",
    "    After that it will put it all togather in a Pandas dataframe and return the dataframe evantually\n",
    "    \n",
    "    '''\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load the page {topics_url}')\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topics_dict = {\n",
    "        'title' : get_topic_title(doc),\n",
    "        'description' : get_topic_description(doc),\n",
    "        'url' : get_topic_url(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ef34d",
   "metadata": {},
   "source": [
    "## Get the top repositories from the topic page\n",
    "\n",
    "- Now we have a dataframe having topic title, description and url in it\n",
    "- So, let's create a function that will iterate over the dataframe and will get the top repos for each topic in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be0869f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    '''\n",
    "    This function will get the topic url, download the HTML content of that topic page,\n",
    "    parse it to the BeautifulSoup and return the returns the formated page with data type of \"bs4.BeautifulSoup\"\n",
    "    \n",
    "    ''' \n",
    "    response = requests.get(topic_url)\n",
    "    #Check Successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load the page {topic_url}')\n",
    "    #Paerse using beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80108951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    return int(float(stars_str[:-1])*1000) if stars_str[-1] == 'k' else int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "07599f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tags ):\n",
    "    '''\n",
    "    This function will get the username(for each repo) which is available in H3 tage\n",
    "    It will also get the star rating for each repo as well\n",
    "    \n",
    "    '''\n",
    "    base_url = 'https://github.com'\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tags.text.strip())\n",
    "    return username, repo_name , stars , repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a150ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    '''\n",
    "    Get h3 tags containing repo title, repo url and username. It will return a dataframe\n",
    "    containing, User Name, Repo Name, Stars and Repo URL for each title \n",
    "    \n",
    "    '''\n",
    "    repo_tags = topic_doc.find_all('h3', {'class' : 'f3 color-text-secondary text-normal lh-condensed'})\n",
    "    #Get star tags\n",
    "    star_tags = topic_doc.find_all('a' , {'class' : 'social-count float-none'})\n",
    "    \n",
    "    topics_repo_dict = {\n",
    "    'User Name' : list(),\n",
    "    'Repo Name' : list(),\n",
    "    'Stars'     : list(),\n",
    "    'Repo URL'  : list()\n",
    "    }\n",
    "    \n",
    "    #Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topics_repo_dict['User Name'].append(repo_info[0])\n",
    "        topics_repo_dict['Repo Name'].append(repo_info[1])\n",
    "        topics_repo_dict['Stars'].append(repo_info[2])\n",
    "        topics_repo_dict['Repo URL'].append(repo_info[3])\n",
    "    return pd.DataFrame(topics_repo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dd009e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        print(f'The file {path} already exists, skipping...')\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    return topic_df.to_csv(path , index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f89f836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    '''\n",
    "    This function is a complete function which will get the topic url and name of the directory to save the data in.\n",
    "    Then it is going to get the dataframe of all the topics after that, it will search the best repos for each topic\n",
    "    in the dataframe. And eventually it is going to save the dataframe for each topic in seperate csv file. \n",
    "\n",
    "    '''\n",
    "    \n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f\"Scarping top repos for {row['title']}\")\n",
    "        scrape_topic(row['url'], f\"data/{row['title']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "37cd26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scarping top repos for 3D\n",
      "Scarping top repos for Ajax\n",
      "Scarping top repos for Algorithm\n",
      "Scarping top repos for Amp\n",
      "Scarping top repos for Android\n",
      "Scarping top repos for Angular\n",
      "Scarping top repos for Ansible\n",
      "Scarping top repos for API\n",
      "Scarping top repos for Arduino\n",
      "Scarping top repos for ASP.NET\n",
      "Scarping top repos for Atom\n",
      "Scarping top repos for Awesome Lists\n",
      "Scarping top repos for Amazon Web Services\n",
      "Scarping top repos for Azure\n",
      "Scarping top repos for Babel\n",
      "Scarping top repos for Bash\n",
      "Scarping top repos for Bitcoin\n",
      "Scarping top repos for Bootstrap\n",
      "Scarping top repos for Bot\n",
      "Scarping top repos for C\n",
      "Scarping top repos for Chrome\n",
      "Scarping top repos for Chrome extension\n",
      "Scarping top repos for Command line interface\n",
      "Scarping top repos for Clojure\n",
      "Scarping top repos for Code quality\n",
      "Scarping top repos for Code review\n",
      "Scarping top repos for Compiler\n",
      "Scarping top repos for Continuous integration\n",
      "Scarping top repos for COVID-19\n",
      "Scarping top repos for C++\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd1933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
